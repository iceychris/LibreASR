{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `LibreASR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libreasr.lib.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfgs = [\"./config/base.yaml\", \"./config/contrastive-stft.yaml\"]\n",
    "cfgs = [\"./config/base.yaml\"]\n",
    "conf, lang, builder_train, builder_valid, db, m, learn = parse_and_apply_config(path=cfgs)\n",
    "(conf, lang, db, m, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_train.print()\n",
    "builder_valid.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_train.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl = db.one_batch()\n",
    "X, Ym, _, _ = tpl[0]\n",
    "Y, Y_lens, X_lens = tpl[1]\n",
    "what(X), what(X_lens), what(Y), what(Y_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_lens, Y_lens\n",
    "# m(cudaize(tpl[0])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Augmentation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.aug(n=3000, benchmark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# mpl.rcParams['figure.dpi'] = 400\n",
    "# db.aug(n=5, benchmark=False, only=-3) #0: signal, 7: spectro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.aug(n=5)#, only=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = conf[\"cuda\"][\"device\"]\n",
    "m = m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)\n",
    "print(\"Encoder   n_params:\", n_params(m.encoder))\n",
    "# print(\"Predictor n_params:\", n_params(m.predictor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_in = db.one_batch()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m.eval()\n",
    "with torch.no_grad():\n",
    "    _out = m(cudaize(_in, device), return_loss=False)\n",
    "    print(\"X\", _in[0].shape, \"->\", _out.shape)\n",
    "    print(\"Y\", _in[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Train`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Load from Checkpoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False # TRUE   ::::::::::::::::::::::::::::::::::::::::::::::::!!!!!!!!!!!! ! ! ! !\n",
    "load_with_opt = True\n",
    "load_strict = True\n",
    "load_encoder = False\n",
    "load_encoder_file = \"models/TRAINED/multi-contrastive-11ep-0.159val-wandb-zw7fv16h.pth\"\n",
    "print_tb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"best_wer\"\n",
    "ckpt = \"libreasr-de-24-12-2020\"\n",
    "ckpt = \"model\"\n",
    "ckpt_load = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before Load:\", m.encoder.input_norm.weight[0].item())\n",
    "\n",
    "if load:\n",
    "    ns = learn.model\n",
    "    try:\n",
    "        if conf[\"training\"][\"noisystudent\"]:\n",
    "            learn.model = ns.teacher[0]\n",
    "            learn.load(ckpt_load, with_opt=False, strict=False)\n",
    "            learn.model = ns\n",
    "            learn.create_opt()\n",
    "        else:\n",
    "            learn.load(ckpt_load, with_opt=load_with_opt, strict=load_strict) # OPT TRUE\n",
    "    except Exception as e:\n",
    "        print(\"failed loading ckpt\", ckpt)\n",
    "        print(e)\n",
    "    finally:\n",
    "        if conf[\"training\"][\"noisystudent\"]:\n",
    "            learn.model = ns\n",
    "            learn.create_opt()\n",
    "if load_encoder:\n",
    "    s = torch.load(load_encoder_file)\n",
    "    sm = s['model']\n",
    "    # sm = s\n",
    "    d = sm\n",
    "    \n",
    "    # fix preprocessor / spec\n",
    "    n = {}\n",
    "    for k in d.keys():\n",
    "        if k.startswith(\"spec\"):\n",
    "            n[\"preprocessor.\" + k] = d[k]\n",
    "    d.update(n)\n",
    "    \n",
    "    # filter out predictor & spec\n",
    "    print(\"raw:\", list(d.keys()))\n",
    "    d = {k: v for k, v in d.items() if not k.startswith(\"latents\")}\n",
    "    d = {k: v for k, v in d.items() if not k.startswith(\"temperature\")}\n",
    "    d = {k: v for k, v in d.items() if not k.startswith(\"predictor\")}\n",
    "    # d = {k: v for k, v in d.items() if not k.startswith(\"preprocessor\")}\n",
    "    # d = {k: v for k, v in d.items() if not k.startswith(\"encoder.input_norm\")}\n",
    "    # d = {k: v for k, v in d.items() if not k.startswith(\"encoder.rnn_stack.rnns.0.weight_ih_l0\")}\n",
    "    # d = {k: v for k, v in d.items() if not k.startswith(\"joint\")}\n",
    "    print(\"filtered:\", list(d.keys()))\n",
    "    \n",
    "    m.load_state_dict(d, strict=False)\n",
    "    \n",
    "print(\"After  Load:\", m.encoder.input_norm.weight[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for later\n",
    "# learn.save(load_encoder_file.replace(\".pth\", \"-pretrained\").replace(\"models/\", \"\"), with_opt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check forward and backward\n",
    "# m(cudaize(_in)).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.save(\"base\", with_opt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# m.eval()\n",
    "# with torch.no_grad():\n",
    "#     if conf[\"training\"][\"noisystudent\"]:\n",
    "#         _out = m.teacher[0].transcribe_batch(cudaize(_in))\n",
    "#     else:\n",
    "#         _out = m.transcribe_batch(cudaize(_in))\n",
    "# _ = [print(x) for x in _out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# m.train()\n",
    "# _out = m(cudaize(_in))\n",
    "# _out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DistributedDataParallel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup if necessary\n",
    "ddp = conf.get(\"training\", {}).get(\"ddp\", {}).get(\"enable\", False)\n",
    "ctx = contextlib.suppress\n",
    "if ddp:\n",
    "    from fastai.distributed import DistributedDL\n",
    "    \n",
    "    # fix -1s (required for sending around idxs)\n",
    "    @patch\n",
    "    def get_idxs(self: DistributedDL):\n",
    "        idxs = self.dl.get_idxs()       # compute get_idxs in all ranks (we'll only use rank 0 but size must be consistent)\n",
    "        idxs = self._broadcast(idxs,0)  # broadcast and receive it from rank 0 to all\n",
    "        # filter out -1s introduced in DynamicBucketingDL\n",
    "        if isinstance(idxs[0], (list, tuple)):\n",
    "            idxs = list(map(lambda s: list(filter(lambda x: x != -1, s)), idxs))\n",
    "        self.n = len(idxs)              # we assumed n was dl.n but we really care about number of idxs\n",
    "        # add extra samples to make it evenly divisible\n",
    "        self.n_padded = DistributedDL._round_to_multiple(self.n,self.world_size)\n",
    "        idxs += (idxs * (self.n_padded//self.n))[:self.n_padded-self.n] # idx needs to be repeated when n_padded>>n\n",
    "        # slice padded idxs so that each rank gets self.n_padded//self.world_size tensors\n",
    "        return idxs[self.rank*self.n_padded//self.world_size:(self.rank+1)*self.n_padded//self.world_size]\n",
    "    \n",
    "    dev = torch.device(\"cuda:\" + str(rank_distrib()))\n",
    "    learn.remove_cb(DeviceCallback)\n",
    "    learn.add_cb(DeviceCallback(dev))\n",
    "    ctx = learn.distrib_ctx\n",
    "    print(\"DistributedDataParallel activated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(num_it=400)\n",
    "# learn.lr_find(num_it=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypers\n",
    "do_warmup = True\n",
    "do_tuning = True\n",
    "do_cyclical = False\n",
    "epochs = 50\n",
    "warmup_epochs = 3.9\n",
    "warmup = int(warmup_epochs + 1)\n",
    "\n",
    "# initial fit: 5e-4 / (1e-3 for large batch)\n",
    "# cat runs:    3e-4\n",
    "lr = 1e-3 # 5e-4 # 3e-4 # 5e-4 # 7e-4 # 5e-4 # 3e-4\n",
    "wd = 0.0 # 0.01\n",
    "\n",
    "div = 500. # 100.\n",
    "div_final = 1.001\n",
    "\n",
    "pct_start = warmup_epochs / warmup\n",
    "device = conf[\"cuda\"][\"device\"]\n",
    "\n",
    "# cyclical stage\n",
    "cyc_epochs = 3\n",
    "cyc_b = 1\n",
    "cyc_lr = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_safe(func, *args, **kwargs):\n",
    "    try:\n",
    "        func(*args, **kwargs)\n",
    "    except:\n",
    "        print(learn.recorder.values)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "def fix_opt():\n",
    "    learn.save(\"tmp\", with_opt=True)\n",
    "    learn.load(\"tmp\", with_opt=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ctx():\n",
    "    # WARMUP\n",
    "    print(learn.cbs)\n",
    "    if do_warmup:\n",
    "        print(\"STAGE: WARMUP\")\n",
    "        fit_safe(learn.fit_one_cycle, warmup, lr, wd=wd, cbs=learn.extra_cbs, div_final=div_final, div=div, pct_start=pct_start, moms=(0.95,)*3)\n",
    "        fix_opt()\n",
    "\n",
    "    # TUNING\n",
    "    if do_tuning:\n",
    "        print(\"STAGE: TUNING\")\n",
    "        fit_safe(learn.fit, epochs, lr, wd=wd, cbs=learn.extra_cbs)\n",
    "        fix_opt()\n",
    "        \n",
    "    # CYCLICAL\n",
    "    if do_cyclical:\n",
    "        print(\"STAGE: CYCLICAL\")\n",
    "        fit_safe(learn.fit_sgdr, cyc_epochs, cyc_b, cyc_lr, wd=wd, cbs=learn.extra_cbs)\n",
    "        fix_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `checks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model for later\n",
    "# learn.save(checkpoints[-1], with_opt=True)\n",
    "learn.save(ckpt, with_opt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"multi-pretrained\", with_opt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Quantization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libreasr.lib.quantization import quantize_rnnt, save_quantized, load_quantized\n",
    "\n",
    "# quantize\n",
    "m8 = quantize_rnnt(m, {Encoder, Predictor, Joint})\n",
    "\n",
    "# save\n",
    "save_quantized(m8)\n",
    "\n",
    "# load\n",
    "m9 = load_quantized(m, lang)\n",
    "m9.lang = lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(mod):\n",
    "    # mod = mod.eval()\n",
    "    with torch.no_grad():\n",
    "        q = mod.transcribe_batch(_in)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "infer(m9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model = m9\n",
    "# learn.model = m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Decode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampls = 2000 # 99999999\n",
    "test_name = \"dev-cv\" # \"dev-all-cat-mix-cont\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = list(eval_speech_model(learn, min_samples=sampls, train=False, save_best=False, pcent=1.0)) # , device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylens = map(lambda x: x.get(\"text/ground_truth\"), _)\n",
    "ylens = filter(lambda x: x is not None and len(x) != 0, ylens)\n",
    "ylens = map(lambda x: len(x), ylens)\n",
    "ylens = np.array(list(ylens))\n",
    "\n",
    "wers = map(lambda x: x.get(\"metrics/wer\", None), _)\n",
    "wers = filter(lambda x: x is not None, wers)\n",
    "wers = np.array(list(wers))\n",
    "ylens[:3], wers[:3], len(ylens), len(wers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = np.stack([ylens, wers]).T\n",
    "stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(stack, columns=[\"ylen\", \"wer\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_wer = df.groupby(\"ylen\").mean()[\"wer\"].values\n",
    "std_wer = df.groupby(\"ylen\").std()[\"wer\"].values\n",
    "border_upper = mean_wer + std_wer / 2\n",
    "border_lower = mean_wer - std_wer / 2\n",
    "count = df.groupby(\"ylen\").count()[\"wer\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plot_wer = fig.add_subplot(111)\n",
    "plot_count = plot_wer.twinx()\n",
    "x = np.arange(len(mean_wer))\n",
    "\n",
    "# wer\n",
    "plot_wer.plot(mean_wer)\n",
    "plot_wer.set_ylim(0.0, 1.2)\n",
    "plot_wer.set_ylabel(\"WER\")\n",
    "plot_wer.set_xlabel(\"Label Length\")\n",
    "\n",
    "# std\n",
    "plot_wer.fill_between(x, border_upper, border_lower, alpha=0.2)\n",
    "\n",
    "# count\n",
    "plot_count.bar(x, count, alpha=0.1)\n",
    "plot_count.set_ylabel(\"Number of Examples\")\n",
    "plot_count.set_yscale(\"log\")\n",
    "\n",
    "# save plot\n",
    "# plt.savefig(f\"{test_name}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tests\n",
    "import pickle\n",
    "pickle.dump(_, open(f'{test_name}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sort(results, rev):\n",
    "    results = filter(lambda x: \"text/ground_truth\" in list(x.keys()), results)\n",
    "    results = sorted(results, key=lambda x: x[\"metrics/wer\"], reverse=rev)\n",
    "    return results\n",
    "\n",
    "def best(results):\n",
    "    return _sort(results, rev=False)\n",
    "\n",
    "def worst(results, n=25):\n",
    "    results = _sort(results, rev=True)[:n]\n",
    "    df = builder_valid.df\n",
    "    for res in results:\n",
    "        def filter_fn(row):\n",
    "            s1 = (res[\"text/ground_truth\"])\n",
    "            s2 = sanitize_str(row.label.decode(\"utf-8\"))\n",
    "            in_ok = s1 in s2\n",
    "            l_ok = abs(len(s1) - len(s2)) < 10\n",
    "            return in_ok and l_ok\n",
    "        rows = df[df.apply(filter_fn, axis=1)]\n",
    "        row = rows\n",
    "        print(len(row))\n",
    "        if len(row) >= 1:\n",
    "            row = row.iloc[0]\n",
    "            file = row.file\n",
    "            sr = int(row.sr)\n",
    "            xstart = int(row.xstart / 1000. * sr)\n",
    "            xlen = int(row.xlen / 1000. * sr)\n",
    "            data, sr = torchaudio.load(file, frame_offset=xstart, num_frames=xlen)\n",
    "            print(\"PRED:\", res[\"text/prediction\"])\n",
    "            print(\"TRUE:\", res[\"text/ground_truth\"])\n",
    "            print(f\"WER:  {res['metrics/wer'] * 100.:.2f}%\")\n",
    "            display(Audio(data.numpy(), rate=sr))\n",
    "        else: print(\"err\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst(_, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.joint.joint[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.joint.joint[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"it-best-noopt\", with_opt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "for batch, _ in zip(db.valid, range(2)):\n",
    "    batches.append(batch)\n",
    "\n",
    "bv = batches[0][0]\n",
    "bv[0].shape, bv[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some values to zero\n",
    "# to check if similarity is still okish\n",
    "X, Y = bv[0].size(1), bv[1].size(1)\n",
    "bv[0][:, :X // 10] = 0\n",
    "bv[1][:, :Y // 10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveAnalyzer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.batch = None\n",
    "        self.outputs = None\n",
    "        self.text = None\n",
    "        self.mat = None\n",
    "    \n",
    "    def rep(self, b, plot=False, save=False, dpi=300):\n",
    "        self.batch = b\n",
    "        m = self.model\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            e, p, a, labels = m(cudaize(b), return_logits=True)\n",
    "            self.outputs = (e, p, a, labels)\n",
    "            print(\"Loss:\", nn.CrossEntropyLoss()(a, labels).item())\n",
    "        if plot:\n",
    "            import matplotlib as mpl\n",
    "            mpl.rcParams['figure.dpi'] = dpi\n",
    "            plt.imshow(a.cpu().numpy())\n",
    "            plt.colorbar()\n",
    "            plt.ylabel(\"Transcript\")\n",
    "            plt.xlabel(\"Audio\")\n",
    "            plt.title(f\"Representation Similarity\")\n",
    "            if bool(save):\n",
    "                plt.savefig(save, dpi=300)\n",
    "                \n",
    "    def metrics(self, learn):\n",
    "        from fastai2.metrics import accuracy\n",
    "        accs1 = []\n",
    "        accs2 = []\n",
    "        learn.model.eval()\n",
    "        from tqdm import tqdm\n",
    "        for b in tqdm(learn.dls.valid):\n",
    "            with torch.no_grad():\n",
    "                inp, targ = m(cudaize(b[0]))\n",
    "            a1 = accuracy(inp, targ, axis=0).item()\n",
    "            a2 = accuracy(inp, targ, axis=1).item()\n",
    "            accs1.append(a1)\n",
    "            accs2.append(a2)\n",
    "        return np.array(accs1).mean(), np.array(accs2).mean()\n",
    "            \n",
    "    def texts(self):\n",
    "        # save\n",
    "        l = []\n",
    "        for i, b in enumerate(self.batch[1]):\n",
    "            l.append([i, lang.denumericalize(b.numpy().tolist())])\n",
    "        self.text = l\n",
    "            \n",
    "    def rank(self):\n",
    "        mat = []\n",
    "        l = self.text\n",
    "        from difflib import SequenceMatcher\n",
    "        for i, a in l:\n",
    "            for j, b in l:\n",
    "                if i != j:\n",
    "                    ratio = SequenceMatcher(None, a, b).ratio()\n",
    "                    mat.append([i, j, a, b, ratio])\n",
    "        # sort\n",
    "        mat = sorted(mat, key=lambda x: x[-1], reverse=True)\n",
    "        self.mat = mat\n",
    "        return mat\n",
    "    \n",
    "    def similar(self, n=10):\n",
    "        return self.mat[:n]\n",
    "    \n",
    "    def circles(self):\n",
    "        self.circles = list(filter(lambda x: x[-1] != 1., self.mat))[:10]\n",
    "        for k in self.circles:\n",
    "            print(k[0], k[1])\n",
    "            \n",
    "    def compare(self, p=True, thresh=0.75):\n",
    "        p = self.outputs[1 if p else 0]\n",
    "        sim2 = torch.einsum(\"i d, j d -> i j\", p, p)\n",
    "        sim2[sim2 < thresh] = 0.\n",
    "        plt.imshow(sim2.cpu().numpy())\n",
    "        plt.colorbar()\n",
    "        plt.tick_params(bottom=True)\n",
    "        from matplotlib.ticker import MultipleLocator\n",
    "        ml = MultipleLocator(2)\n",
    "        plt.axes().yaxis.set_minor_locator(ml)\n",
    "        plt.axes().xaxis.set_minor_locator(ml)\n",
    "        \n",
    "    def activations(self, p=True, to=100):\n",
    "        act = self.outputs[1 if p else 0]\n",
    "        plt.imshow(act[:, :to].cpu().numpy())\n",
    "        plt.colorbar()\n",
    "        plt.tick_params(bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ContrastiveAnalyzer(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = c.rep(bv)\n",
    "_ = c.texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.rep(bv, plot=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.compare(p=True, thresh=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.activations(p=False, to=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = c.rank()\n",
    "# c.similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.metrics(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
