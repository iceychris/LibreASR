###
# data
###

transforms:        

  # independent variable
  x-stft:
    - name: MyOpenAudio
      partial: true
      args:
        pad_end: 0.3
        pad_factor: 1.5

    # resample
    - name: Resample
      partial: true
      args:
        resample_sr: 8000

    # augmentation
    - name: ResamplePoly
      wrap: true
      partial: true
      args:
        delta: 10
    - name: MySignalShifter
      wrap: true
      partial: true
      args:
        max_time: 0.1
        direction: 1
 
    # bounds
    - name: PadderCutter
 
    # prepare for batching
    - name: FixRawAudio

  # independent variable
  x-no-stft:
    - name: MyOpenAudio
      partial: true
      args:
        pad_end: 0.5
        pad_factor: 2.25
    # - name: ChannelCut

    # # resampling
    # - name: Resample

    # augmentation
    # - name: ResamplePoly
    #   wrap: true
    #   partial: true
    #   args:
    #     delta: 20
    # - name: ChangeVolume
    #   wrap: true
    #   partial: true
    #   args:
    #     pcent: 0.03
    # - name: MyAddNoise
    #   wrap: true
    #   partial: true
    #   args:
    #     noise_level: 0.05
    #     color: 0
    # - name: MySignalShifter
    #   wrap: true
    #   partial: true
    #   args:
    #     max_time: 0.1
    #     direction: 1
    # - name: PadderCutter
    - name: TransformTime
    - name: MyCutFrames
      wrap: true
      partial: true
      args:
        max_front: 1
        max_back: 1
    - name: MyMaskTime
      wrap: true
      partial: true
      args:
        num_masks: 2
        size: 4
    - name: MyMaskFreq
      wrap: true
      partial: true
      args:
        num_masks: 4
        size: 4

    # reduce time dimension
    #  by downsample
    # and stack
    - name: StackDownsample
      partial: true
      args:
        downsample: 8
        n_stack: 10

    # make data ready for batching
    - name: FixDimensions

  # dependent variable
  y:
    - name: MyOpenLabel
    - name: PadCutLabel
    - name: MyNumericalize
    - name: AddLen

# set the used language
#  "multi" for multilingual
lang: de

# datasets
dataset_paths: 
    tf-speech: "/data/stt/data/tf-speech-recognition/train"
    tatoeba-en: "/data/stt/data/tatoeba/en"
    librispeech-0: "/data/stt/data/librispeech/audio/LibriSpeech/train-clean-100"
    librispeech-1: "/data/stt/data/librispeech/audio/LibriSpeech/train-clean-360"
    librispeech-2: "/data/stt/data/librispeech/audio/LibriSpeech/train-other-500"
    common-voice-en: "/data/stt/data/common-voice/en"
    common-voice-de: "/data/stt/data/common-voice/de"
    common-voice-fr: "/data/stt/data/common-voice/fr"
    common-voice-es: "/data/stt/data/common-voice/es"
    common-voice-it: "/data/stt/data/common-voice/it"
    common-voice-et: "/data/stt/data/common-voice/et"
    yt-en: "/data/stt/data/yt/en"
    yt-de: "/data/stt/data/yt/de"
    yt-fr: "/data/stt/data/yt/fr"
    yt-es: "/data/stt/data/yt/es"
    yt-it: "/data/stt/data/yt/it"
    et-a: "/data/stt/data/estonian/intervjuud"
    et-b: "/data/stt/data/estonian/vestlussaated"
    et-c: "/data/stt/data/estonian/uudised"
    et-d: "/data/stt/data/estonian/loengusalvestused"
    et-e: "/data/stt/data/estonian/riigiokgu"
    et-f: "/data/stt/data/estonian/jutusaated"

# selected datasets
datasets:
  en:
    train:
      - common-voice-en
      - yt-en
    valid:
      - common-voice-en
      # - yt-en
  de:
    train:
      - common-voice-de
      - yt-de
    valid:
      - common-voice-de
      # - yt-de
  fr:
    train:
      - common-voice-fr
      - yt-fr
    valid:
      - common-voice-fr
      # - yt-fr
  es:
    train:
      - common-voice-es
      - yt-es
    valid:
      - common-voice-es
      # - yt-es
  it:
    train:
      - common-voice-it
      - yt-it
    valid:
      - common-voice-it
      # - yt-it
  et:
    train:
      - common-voice-et
      - et-a
      - et-b
      - et-c
      - et-d
      - et-e
      - et-f
    valid:
      - common-voice-et

# percentage of each individual dataframe used
#  (stage: builder)
pcent:
    train: 1.0  # 0.25 # 1.0
    valid: 1.0  # 1.0
    test: 1.0


###
# audio & spectrogram
###

sr: 16000
channels: 1
mfcc: True
mfcc_args: {}
melkwargs:
  n_fft: 1024
  n_mels: 128

# seconds
win_length: 0.025
hop_length: 0.01

# N frames
delta_win_length: 3
deltas: 0

# concat N future frames
n_forward_frames: 0


###
# labels
###

tokenizer:
  model_file: "./tmp/tokenizer.yttm-model"


###
# data
###

# apply the following limits
# at the builder stage?
apply_limits: True

# ------------- regular training   -------------
# audio length [seconds]
almins: 0.5
almaxs: 15.0 # 8.0 # 10.0 # 8.0 # 6.0 # 9.0

# label length [characters]
y_min: 1
y_max: 300 # 120 # 80 # 130 # 80 # 60 # 100
y_max_words: 200

# ------------- streaming training -------------
# almins: 0.5
# almaxs: 80.0
# y_min: 1
# y_max: 15000
# y_max_words: 1000


# which dataset to use
suffix: "" # "cat-2468-a" # "" # ["cat-2468-a", "cat-2468-b", "cat-2468-c"]

# dataloader settings
dataloader_args:

  # maximum batch size
  bs_max: 96 # 64 # 48 # 128 # 64 # 32

  # batch size multiplier
  bs_mul: 3. # 2. # 1.5 # 4. # 2. # 1.

  # prng save state path
  prng_state_path: "/home/chris"

  # dataloader debugging
  debug: false


###
# GPU options
###

cuda:
  enable: true

  # use cudnn benchmark for better runtime perf
  benchmark: false

  # set default cuda device (to avoid errors?)
  # see: https://discuss.pytorch.org/t/runtime-error-77/9392/9?u=sbelharbi
  device: "cuda:0"


###
# CPU options
###

cpu:
  threads: 4


###
# model
###

model:
  name: Transducer
  feature_sz: 1280
  embed_sz: 512
  hidden_sz: 1024
  out_sz: 1024
  joint_sz: 1024
  vocab_sz: 2048
  preprocessor:
    sr: 8000
    n_mels: 80 # 64
    n_fft: 1024
    downsample: 16
    stack: 16
    trainable: true
  encoder:
    name: Encoder # LinearTransformerEncoder
    rnn_type: "LSTM"
    num_layers: 6 # 16
    norm: "ln"
    dropout: 0.1
    dropout_input: 0.0
    dropout_inner: 0.0
    reduction_factor: 1
    reduction_factors: []
    attention: false
    use_tmp_state_pcent: 0.0 # 0.95
  predictor:
    name: Predictor
    rnn_type: "LSTM"
    num_layers: 2
    norm: "ln"
    dropout: 0.05
    use_tmp_state_pcent: 0.0 # 0.95
  joint:
    enable: true
    method: "concat"
    reversible: false # true
    dropout: 0.0
  use_tmp_bos: false
  use_tmp_bos_pcent: 0.2
  learnable_stft: true
  loss: true


###
# batching
###

# DEPRECATED
# feature-wise norm over batch
norm_file: "./tmp/norm.pt"

# ascending data length?
ascending: True

# whether to sort batches (order) by size or not
#  DataLoader
shuffle: True

# shuffle at builder stage
shuffle_builder:
    train: True
    valid: True
    test: True


###
# training
###

training:
  optimizer: Ranger # AdaHessian

  # use noisy student training
  #  (a variant of knowledge distillation)
  noisystudent: false

  # Gradient Clipping (L2-Norm)
  #  (set to 0.0 to disable)
  clip-grad-norm: 2.0

  # NativeMixedPrecision training
  mp:
    enable: false

  # DistributedDataParallel
  #  (muli-gpu and/or multi-host training)
  ddp:
    enable: false


###
# other
###

# batch management
batching:

  # batch size for training dataset
  #  (managed by DynamicBucketingDL)
  batch_size_train: "auto"

  # batch size for validation dataset
  batch_size_valid: 8

  # gradient accumulation (gas)
  #  (number of batches)
  #  (regular training: 20)
  #  (DDP training: 10)
  # streaming: 50
  accumulate: 32 # 20 # 10 # 5

# losses
loss:
  type: "rnnt"

## tokenization
# for training a tokenizer on the current corpus
train_tokenizer: false
dump_labels: false
wanted_vocab_sz: 2048

# random seed
seed: 42

# CPU workers
#  3 -> fine for rnnt
num_workers: 6 # 3 # 6 # 7

# testing
tensorboard: true
wandb: true
tests_per_epoch: 8 # 24 # 8 # 24

# eval speech model kwargs
espm_kwargs:
  pcent: 1.0
  min_samples: 800
  save_best: true
  save_with_opt: true
  save_multiple: true

# q
quantization:
  engine: "fbgemm" # "qnnpack" or "fbgemm"
  model:
    pre: false
    post: false
  lm:
    pre: false
    post: false

lm:
  enable: false
